{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:14:07.422006Z",
     "start_time": "2025-06-28T10:14:03.968205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "from synthetic_data.synthetic_data_for_cate_class import synthetic_data_for_cate\n",
    "# Force reload of the t_learner module to ensure we're using the latest version\n",
    "import metalearners.t_learner\n",
    "importlib.reload(metalearners.t_learner)\n",
    "from metalearners.t_learner import TLearner\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set plot style\n",
    "try:\n",
    "    # For newer matplotlib versions (>=3.6)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except ValueError:\n",
    "    # For older matplotlib versions\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Try to import optional dependencies\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not available. Some examples will be skipped.\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Some examples will be skipped.\")\n"
   ],
   "id": "842dd9e3118939b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Note on Dependencies\n",
    "\n",
    "This notebook requires the following packages:\n",
    "- numpy, matplotlib, seaborn: For data manipulation and visualization\n",
    "- scikit-learn: For machine learning models\n",
    "- lightgbm, xgboost (optional): For gradient boosting models\n",
    "\n",
    "The notebook also uses local packages:\n",
    "- synthetic_data.synthetic_data_for_cate: For generating synthetic data with enhanced heterogeneity\n",
    "- metalearners.t_learner: For the TLearner implementation\n",
    "\n",
    "Make sure you have installed all required dependencies before running this notebook.\n",
    "</div>\n"
   ],
   "id": "9a4fe36d4d7d16f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Synthetic Data Generation\n",
    "\n",
    "We generate synthetic data with known heterogeneous treatment effects using the `synthetic_data_for_cate` class. This function creates:\n",
    "\n",
    "- A feature matrix with 5 covariates (by default)\n",
    "- A binary treatment indicator (1=treated, 0=control)\n",
    "- An outcome variable that depends on both covariates and treatment\n",
    "\n",
    "The data generation process includes:\n",
    "- Non-linear confounding (treatment assignment depends on X1 and X2)\n",
    "- Heterogeneous treatment effects (effects vary based on all covariates)\n",
    "- Non-linear baseline effects (outcome depends non-linearly on covariates)\n",
    "- Heteroskedastic noise (noise level varies with X1)\n",
    "\n",
    "This synthetic data allows us to evaluate the performance of different CATE estimation methods, as we know the true treatment effects.\n",
    "</div>\n"
   ],
   "id": "d982640a0ae4174e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:14:08.069859Z",
     "start_time": "2025-06-28T10:14:08.065653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an instance of synthetic_data_for_cate with model2\n",
    "data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "# Generate synthetic data with heterogeneous treatment effects\n",
    "features, treatment_vector, outcomes = data_generator.get_synthetic_data()\n",
    "\n",
    "# Print basic information about the generated data\n",
    "print(f\"Generated data with {features.shape[0]} samples and {features.shape[1]} features\")\n",
    "print(f\"Treatment assignment rate: {treatment_vector.mean():.2f}\")\n",
    "print(f\"Outcome mean: {outcomes.mean():.2f}\")\n"
   ],
   "id": "ece71a2778b5c0cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Model Initialization and Fitting\n",
    "\n",
    "We initialize several T-Learner models with different base learners:\n",
    "\n",
    "1. **Random Forest**: A non-parametric model that can capture complex non-linear relationships\n",
    "2. **Linear Regression**: A simple parametric model that assumes linear relationships\n",
    "3. **Gradient Boosting**: An ensemble method that builds trees sequentially to correct errors\n",
    "4. **LightGBM** (if available): A gradient boosting framework that uses tree-based learning\n",
    "5. **XGBoost** (if available): Another gradient boosting framework with different implementation\n",
    "\n",
    "Each model has different strengths and weaknesses for CATE estimation. For this demonstration, we'll use the Random Forest-based T-Learner as our primary model.\n",
    "\n",
    "The fitting process involves:\n",
    "1. Splitting the data into treatment and control groups\n",
    "2. Training separate models for each group\n",
    "3. Using these models to predict outcomes under different treatment conditions\n",
    "</div>\n"
   ],
   "id": "211569d97016ec22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:14:08.396920Z",
     "start_time": "2025-06-28T10:14:08.111064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize T-Learner with different base models\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "linear_model = LinearRegression()\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create T-Learner instances with different models\n",
    "t_learner_rf = TLearner(rf_model)\n",
    "t_learner_linear = TLearner(linear_model)\n",
    "t_learner_gb = TLearner(gb_model)\n",
    "\n",
    "# Create LightGBM model if available\n",
    "if 'LIGHTGBM_AVAILABLE' in globals() and LIGHTGBM_AVAILABLE:\n",
    "    lgbm_model = LGBMRegressor(random_state=42)\n",
    "    t_learner_lgbm = TLearner(lgbm_model)\n",
    "    # Uncomment to use LightGBM model\n",
    "    # tl = t_learner_lgbm\n",
    "\n",
    "# Create XGBoost model if available\n",
    "if 'XGBOOST_AVAILABLE' in globals() and XGBOOST_AVAILABLE:\n",
    "    xgb_model = XGBRegressor(random_state=42)\n",
    "    t_learner_xgb = TLearner(xgb_model)\n",
    "    # Uncomment to use XGBoost model\n",
    "    # tl = t_learner_xgb\n",
    "\n",
    "# Select Random Forest as our primary model\n",
    "tl = t_learner_rf\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting T-Learner with Random Forest...\")\n",
    "tl.fit(\n",
    "    X=features,             # Feature matrix\n",
    "    t=treatment_vector,     # Treatment assignments (0/1)\n",
    "    y=outcomes              # Observed outcomes\n",
    ")\n",
    "print(\"Model fitting complete.\")\n"
   ],
   "id": "74386d5ada650c9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Estimation\n",
    "\n",
    "After fitting the model, we can estimate the Conditional Average Treatment Effect (CATE) for each individual in our dataset. The CATE represents how much the treatment is expected to affect the outcome for an individual with specific characteristics.\n",
    "\n",
    "The CATE estimation process for T-Learner involves:\n",
    "1. Using the treatment model to predict outcomes if treated\n",
    "2. Using the control model to predict outcomes if not treated\n",
    "3. Taking the difference between these predictions to get the CATE\n",
    "\n",
    "This gives us an estimate of the treatment effect for each individual, conditional on their covariates.\n",
    "</div>\n"
   ],
   "id": "204888133b8a4398"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:14:08.445602Z",
     "start_time": "2025-06-28T10:14:08.419868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Estimate treatment effects (CATE)\n",
    "cate = tl.effect(X=features)\n",
    "\n",
    "# Print basic statistics about the estimated CATE\n",
    "print(f\"CATE Statistics:\")\n",
    "print(f\"  Mean: {cate.mean():.4f}\")\n",
    "print(f\"  Std Dev: {cate.std():.4f}\")\n",
    "print(f\"  Min: {cate.min():.4f}\")\n",
    "print(f\"  Max: {cate.max():.4f}\")\n"
   ],
   "id": "2a34535a179ce14f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Distribution Visualization\n",
    "\n",
    "Visualizing the distribution of estimated treatment effects helps us understand the heterogeneity in treatment effects across the population. This can reveal:\n",
    "\n",
    "1. **Average Effect**: The center of the distribution shows the average treatment effect\n",
    "2. **Effect Heterogeneity**: The spread of the distribution shows how much treatment effects vary\n",
    "3. **Subgroups**: Multiple peaks might indicate distinct subgroups with different responses\n",
    "4. **Negative/Positive Effects**: The proportion of individuals with negative vs. positive effects\n",
    "\n",
    "A narrow distribution suggests homogeneous treatment effects, while a wide distribution suggests high heterogeneity. Skewness in the distribution might indicate that certain types of individuals benefit more or less from the treatment.\n",
    "</div>\n"
   ],
   "id": "28501d08cd6a332d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:14:09.007670Z",
     "start_time": "2025-06-28T10:14:08.458576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate true treatment effects using the data generator\n",
    "true_cate = data_generator.get_true_cate(features)\n",
    "\n",
    "# Create a DataFrame for easier plotting with seaborn\n",
    "import pandas as pd\n",
    "cate_df = pd.DataFrame({\n",
    "    'Estimated CATE': cate,\n",
    "    'True CATE': true_cate\n",
    "})\n",
    "\n",
    "# Create images directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "# Plot the distribution of estimated CATE\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot both distributions with KDE\n",
    "sns.histplot(data=cate_df['Estimated CATE'], kde=True, alpha=0.6, bins=30)\n",
    "\n",
    "# Add vertical lines for mean values\n",
    "plt.axvline(x=cate.mean(), color='blue', linestyle='--', linewidth=2,\n",
    "            label=f'Mean Estimated CATE: {cate.mean():.4f}')\n",
    "plt.axvline(x=true_cate.mean(), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f'Mean True CATE: {true_cate.mean():.4f}')\n",
    "\n",
    "# Add vertical line at zero (no effect)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1,\n",
    "            label='No Effect')\n",
    "\n",
    "# Add annotations\n",
    "plt.title('Distribution of Estimated CATE - T-Learner', fontsize=14)\n",
    "plt.xlabel('Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show percentage of positive and negative effects for estimated CATE\n",
    "est_pos_pct = (cate > 0).mean() * 100\n",
    "est_neg_pct = (cate < 0).mean() * 100\n",
    "plt.annotate(f'Estimated Positive Effects: {est_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.90), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'Estimated Negative Effects: {est_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.85), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "# Show percentage of positive and negative effects for true CATE\n",
    "true_pos_pct = (true_cate > 0).mean() * 100\n",
    "true_neg_pct = (true_cate < 0).mean() * 100\n",
    "plt.annotate(f'True Positive Effects: {true_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.80), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'True Negative Effects: {true_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.75), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display plot\n",
    "plt.savefig('images/t_learner_cate_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"CATE distribution plot saved to images/t_learner_cate_distribution.png\")\n",
    "plt.show()\n"
   ],
   "id": "2f49b94113fe8df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Overlaid CATE Distributions: Estimated vs. True\n",
    "\n",
    "To better compare the estimated and true CATE distributions, we can overlay them on the same plot using different colors. This visualization allows us to:\n",
    "\n",
    "1. **Directly Compare Shapes**: See how closely the estimated distribution matches the true distribution\n",
    "2. **Identify Discrepancies**: Spot areas where the model over- or under-estimates treatment effects\n",
    "3. **Assess Heterogeneity Capture**: Determine if the model captures the true heterogeneity in treatment effects\n",
    "4. **Evaluate Peaks and Modes**: Compare the peaks and modes of both distributions\n",
    "\n",
    "The plot below shows both distributions with different colors, allowing for a direct visual comparison of their shapes, centers, and spreads.\n",
    "</div>\n"
   ],
   "id": "f2c173a1f34bd97f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:14:09.344952Z",
     "start_time": "2025-06-28T10:14:09.027318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a plot that overlays both the estimated and true CATE distributions\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot both distributions with KDE using different colors and line styles\n",
    "sns.kdeplot(data=cate_df, x='Estimated CATE', color='blue', fill=False, linewidth=2.5, linestyle='-',\n",
    "            label=f'Estimated CATE (Mean: {cate.mean():.4f})')\n",
    "sns.kdeplot(data=cate_df, x='True CATE', color='red', fill=False, linewidth=2.5, linestyle='--',\n",
    "            label=f'True CATE (Mean: {true_cate.mean():.4f})')\n",
    "\n",
    "# Add vertical lines for mean values\n",
    "plt.axvline(x=cate.mean(), color='blue', linestyle='-', linewidth=2)\n",
    "plt.axvline(x=true_cate.mean(), color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add vertical line at zero (no effect)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1, label='No Effect')\n",
    "\n",
    "# Add annotations\n",
    "plt.title('Comparison of Estimated vs. True CATE Distributions - T-Learner', fontsize=14)\n",
    "plt.xlabel('Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate and display statistics\n",
    "est_std = cate.std()\n",
    "true_std = true_cate.std()\n",
    "\n",
    "# Calculate correlation and mean absolute error\n",
    "correlation = np.corrcoef(true_cate, cate)[0, 1]\n",
    "mae = np.mean(np.abs(true_cate - cate))\n",
    "\n",
    "# Add statistics annotations\n",
    "plt.annotate(f'Estimated CATE Std: {est_std:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'True CATE Std: {true_std:.4f}', xy=(0.05, 0.90), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'Mean Absolute Error: {mae:.4f}', xy=(0.05, 0.85), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'Correlation: {correlation:.4f}', xy=(0.05, 0.80), xycoords='axes fraction', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display plot\n",
    "plt.savefig('images/t_learner_cate_distributions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"CATE distributions comparison plot saved to images/t_learner_cate_distributions_comparison.png\")\n",
    "plt.show()\n"
   ],
   "id": "a45c493a85dc3dd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Accuracy Evaluation: Predicted vs. Actual Treatment Effects\n",
    "\n",
    "To evaluate the accuracy of our T-Learner model, we can compare the predicted CATE values with the actual treatment effects. This comparison helps us:\n",
    "\n",
    "1. **Assess Model Accuracy**: How well does the model recover the true treatment effects?\n",
    "2. **Identify Patterns in Errors**: Are there systematic biases in the predictions?\n",
    "3. **Compare Treatment Groups**: Do predictions differ in accuracy between treated and control groups?\n",
    "4. **Detect Outliers**: Are there individuals for whom the model predictions are particularly inaccurate?\n",
    "\n",
    "The scatter plot below shows:\n",
    "- Predicted CATE values on the y-axis\n",
    "- True treatment effects on the x-axis\n",
    "- Different symbols for treated and control groups\n",
    "- A diagonal line representing perfect prediction (y=x)\n",
    "\n",
    "Points close to the diagonal line indicate accurate predictions, while deviations suggest estimation errors.\n",
    "</div>\n"
   ],
   "id": "2e9176f5aafc9ce6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T10:14:09.718524Z",
     "start_time": "2025-06-28T10:14:09.366725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a scatter plot of predicted vs. true treatment effects\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Separate treated and control groups for different markers\n",
    "treated_indices = treatment_vector == 1\n",
    "control_indices = treatment_vector == 0\n",
    "\n",
    "# Plot treated group with one marker\n",
    "plt.scatter(true_cate[treated_indices], cate[treated_indices], \n",
    "            marker='^', color='red', alpha=0.6, label='Treated Group')\n",
    "\n",
    "# Plot control group with another marker\n",
    "plt.scatter(true_cate[control_indices], cate[control_indices], \n",
    "            marker='o', color='blue', alpha=0.6, label='Control Group')\n",
    "\n",
    "# Add diagonal line (perfect prediction)\n",
    "min_val = min(true_cate.min(), cate.min())\n",
    "max_val = max(true_cate.max(), cate.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Perfect Prediction')\n",
    "\n",
    "# Calculate and display correlation coefficient\n",
    "plt.annotate(f'Correlation: {correlation:.4f}', \n",
    "             xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12)\n",
    "\n",
    "# Calculate and display mean absolute error\n",
    "plt.annotate(f'Mean Absolute Error: {mae:.4f}', \n",
    "             xy=(0.05, 0.90), xycoords='axes fraction', fontsize=12)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Predicted vs. Actual Treatment Effects - T-Learner', fontsize=14)\n",
    "plt.xlabel('Actual Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Predicted Treatment Effect (CATE)', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('images/t_learner_cate_accuracy_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"CATE accuracy evaluation plot saved to images/t_learner_cate_accuracy_evaluation.png\")\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "6ebc559123e42688",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Comparing T-Learner with Other Meta-Learners\n",
    "\n",
    "The T-Learner is one of several meta-learner approaches for estimating heterogeneous treatment effects. Each approach has its own strengths and weaknesses:\n",
    "\n",
    "## T-Learner vs. S-Learner\n",
    "- **T-Learner**: Uses separate models for treatment and control groups\n",
    "- **S-Learner**: Uses a single model with treatment as a feature\n",
    "- **When to choose T-Learner**: When treatment and control groups have very different response surfaces\n",
    "- **When to choose S-Learner**: When sample sizes are small or when treatment effects are relatively homogeneous\n",
    "\n",
    "## T-Learner vs. X-Learner\n",
    "- **T-Learner**: Directly estimates outcomes for each group\n",
    "- **X-Learner**: Uses a multi-stage approach with imputed treatment effects\n",
    "- **When to choose T-Learner**: When treatment and control groups are balanced\n",
    "- **When to choose X-Learner**: When treatment and control groups are imbalanced\n",
    "\n",
    "## T-Learner vs. R-Learner\n",
    "- **T-Learner**: Doesn't model the propensity score\n",
    "- **R-Learner**: Uses residualization to separate confounding from treatment effects\n",
    "- **When to choose T-Learner**: When the propensity model is difficult to estimate\n",
    "- **When to choose R-Learner**: When strong confounding is present\n",
    "\n",
    "In practice, it's often valuable to try multiple meta-learners and compare their results, as different approaches may perform better in different scenarios.\n",
    "</div>"
   ],
   "id": "ddb2c74da7cadfda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Library Imports\n",
    "\n",
    "We import the necessary libraries for this demonstration:\n",
    "\n",
    "- **synthetic_data_for_cate2**: Custom module for generating synthetic data with enhanced heterogeneity for treatment effects\n",
    "- **TLearner**: Our implementation of the T-Learner meta-learner\n",
    "- **sklearn models**: Various regression models to use as base learners\n",
    "- **matplotlib/seaborn**: For visualization\n",
    "- **numpy**: For numerical operations\n",
    "\n",
    "We also attempt to import optional dependencies (LightGBM and XGBoost) which provide additional base learners.\n",
    "</div>\n"
   ],
   "id": "513e629d1b198279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T04:23:32.167296Z",
     "start_time": "2025-06-19T04:23:28.732372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from synthetic_data.synthetic_data_for_cate_class import synthetic_data_for_cate\n",
    "from metalearners.t_learner import TLearner\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set plot style\n",
    "try:\n",
    "    # For newer matplotlib versions (>=3.6)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except ValueError:\n",
    "    # For older matplotlib versions\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Try to import optional dependencies\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not available. Some examples will be skipped.\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Some examples will be skipped.\")\n"
   ],
   "id": "e0486608a5d45fa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Note on Dependencies\n",
    "\n",
    "This notebook requires the following packages:\n",
    "- numpy, matplotlib, seaborn: For data manipulation and visualization\n",
    "- scikit-learn: For machine learning models\n",
    "- lightgbm, xgboost (optional): For gradient boosting models\n",
    "\n",
    "The notebook also uses local packages:\n",
    "- synthetic_data.synthetic_data_for_cate2: For generating synthetic data with enhanced heterogeneity\n",
    "- metalearners.t_learner: For the TLearner implementation\n",
    "\n",
    "Make sure you have installed all required dependencies before running this notebook.\n",
    "</div>\n"
   ],
   "id": "6c8a0bfff3b80e5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Synthetic Data Generation\n",
    "\n",
    "We generate synthetic data with known heterogeneous treatment effects using the `generate_synthetic_data_for_cate2()` function. This function creates:\n",
    "\n",
    "- A feature matrix with 5 covariates (by default)\n",
    "- A binary treatment indicator (1=treated, 0=control)\n",
    "- An outcome variable that depends on both covariates and treatment\n",
    "\n",
    "The data generation process includes:\n",
    "- Non-linear confounding (treatment assignment depends on X1 and X2)\n",
    "- Heterogeneous treatment effects (effects vary based on all covariates)\n",
    "- Non-linear baseline effects (outcome depends non-linearly on covariates)\n",
    "- Heteroskedastic noise (noise level varies with X1)\n",
    "\n",
    "This synthetic data allows us to evaluate the performance of different CATE estimation methods, as we know the true treatment effects.\n",
    "</div>\n"
   ],
   "id": "df4e98bb25810405"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T04:23:32.897989Z",
     "start_time": "2025-06-19T04:23:32.893406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an instance of synthetic_data_for_cate with model2\n",
    "data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "# Generate synthetic data with heterogeneous treatment effects\n",
    "features, treatment_vector, outcomes = data_generator.get_synthetic_data()\n",
    "\n",
    "# Print basic information about the generated data\n",
    "print(f\"Generated data with {features.shape[0]} samples and {features.shape[1]} features\")\n",
    "print(f\"Treatment assignment rate: {treatment_vector.mean():.2f}\")\n",
    "print(f\"Outcome mean: {outcomes.mean():.2f}\")\n"
   ],
   "id": "304e03890f701d86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Model Initialization and Fitting\n",
    "\n",
    "We initialize several T-Learner models with different base learners:\n",
    "\n",
    "1. **Random Forest**: A non-parametric model that can capture complex non-linear relationships\n",
    "2. **Linear Regression**: A simple parametric model that assumes linear relationships\n",
    "3. **LightGBM** (if available): A gradient boosting framework that uses tree-based learning\n",
    "4. **XGBoost** (if available): Another gradient boosting framework with different implementation\n",
    "\n",
    "Each model has different strengths and weaknesses for CATE estimation. For this demonstration, we'll use the Random Forest-based T-Learner as our primary model.\n",
    "\n",
    "The fitting process involves:\n",
    "1. Splitting the data into treatment and control groups\n",
    "2. Training separate models on each group\n",
    "</div>\n"
   ],
   "id": "fc0621eb013f8e60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Base Learners for T-Learner: Principles, Pros, and Cons\n",
    "\n",
    "The choice of base learner significantly impacts the performance of T-Learner for CATE estimation. Here we describe the basic ideas, advantages, and limitations of each base learner.\n",
    "\n",
    "## RandomForestRegressor\n",
    "\n",
    "### Basic Principles\n",
    "- **Ensemble Method**: Combines multiple decision trees to improve prediction accuracy and control overfitting\n",
    "- **Bootstrap Aggregating (Bagging)**: Each tree is trained on a random subset of the data with replacement\n",
    "- **Feature Randomization**: At each split, only a random subset of features is considered\n",
    "- **Averaging**: Final prediction is the average of predictions from all trees\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **Captures Non-linear Relationships**: Can model complex, non-linear treatment effects without explicit specification\n",
    "- **Handles Interactions**: Automatically captures interactions between treatment and covariates\n",
    "- **Robust to Outliers**: Less sensitive to extreme values in the data\n",
    "- **No Distributional Assumptions**: Does not assume normality or homoscedasticity\n",
    "- **Feature Importance**: Provides insights into which covariates drive treatment effect heterogeneity\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Black Box**: Less interpretable than linear models\n",
    "- **Treatment Variable Importance**: May not give sufficient importance to the treatment indicator\n",
    "- **Computational Cost**: Training many trees can be computationally intensive\n",
    "- **Hyperparameter Sensitivity**: Performance depends on proper tuning of hyperparameters\n",
    "- **Extrapolation Limitations**: May not extrapolate well to regions with sparse data\n",
    "\n",
    "## LGBMRegressor (LightGBM)\n",
    "\n",
    "### Basic Principles\n",
    "- **Gradient Boosting Framework**: Builds trees sequentially, with each tree correcting errors of previous trees\n",
    "- **Leaf-wise Growth**: Grows trees by leaf-wise (best-first) rather than level-wise growth\n",
    "- **Histogram-based Learning**: Buckets continuous features into discrete bins to speed up training\n",
    "- **Gradient-based One-Side Sampling (GOSS)**: Focuses on instances with larger gradients during training\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **High Performance**: Often achieves state-of-the-art prediction accuracy\n",
    "- **Efficiency**: Faster training speed and lower memory usage than traditional gradient boosting\n",
    "- **Handles Large Datasets**: Scales well to datasets with many observations\n",
    "- **Regularization Options**: Built-in L1/L2 regularization to prevent overfitting\n",
    "- **Categorical Feature Support**: Native handling of categorical variables\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Complex Tuning**: Requires careful hyperparameter tuning for optimal performance\n",
    "- **Overfitting Risk**: Can overfit on small datasets without proper regularization\n",
    "- **Less Robust to Noisy Data**: May be more sensitive to noise than Random Forests\n",
    "- **Treatment Importance**: Like Random Forests, may not give sufficient importance to treatment indicator\n",
    "- **Installation Challenges**: Requires additional system dependencies\n",
    "\n",
    "## XGBRegressor (XGBoost)\n",
    "\n",
    "### Basic Principles\n",
    "- **Extreme Gradient Boosting**: Enhanced implementation of gradient boosting\n",
    "- **Regularized Learning**: Includes L1 and L2 regularization terms in the objective function\n",
    "- **Approximate Greedy Algorithm**: Uses a quantile sketch algorithm to find approximate best splits\n",
    "- **Sparsity-Aware Split Finding**: Efficiently handles missing values and sparse data\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **Prediction Accuracy**: Consistently strong performance across many prediction tasks\n",
    "- **Regularization**: Built-in mechanisms to prevent overfitting\n",
    "- **Handles Missing Values**: Native support for missing data\n",
    "- **Parallel Processing**: Efficient computation using multi-threading\n",
    "- **Cross-validation**: Built-in cross-validation capabilities\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Complexity**: Many hyperparameters to tune\n",
    "- **Black Box Nature**: Limited interpretability of the model\n",
    "- **Memory Usage**: Can be memory-intensive for large datasets\n",
    "- **Treatment Variable Importance**: May not prioritize treatment indicator appropriately\n",
    "- **Installation Requirements**: Depends on additional libraries\n",
    "\n",
    "## LinearRegression\n",
    "\n",
    "### Basic Principles\n",
    "- **Linear Function**: Models outcome as a weighted sum of input features\n",
    "- **Ordinary Least Squares (OLS)**: Minimizes the sum of squared differences between observed and predicted values\n",
    "- **Closed-form Solution**: Parameters can be calculated directly without iterative optimization\n",
    "- **Additive Effects**: Assumes features contribute independently to the outcome\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **Interpretability**: Clear interpretation of coefficients, including treatment effect\n",
    "- **Treatment Interaction Modeling**: Can explicitly model interactions between treatment and covariates\n",
    "- **Computational Efficiency**: Fast to train and make predictions\n",
    "- **Stability**: Results are stable and reproducible\n",
    "- **Statistical Inference**: Provides p-values and confidence intervals for treatment effects\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Linearity Assumption**: Cannot capture non-linear treatment effects without manual feature engineering\n",
    "- **Homogeneity Assumption**: Assumes constant error variance across all observations\n",
    "- **Sensitivity to Outliers**: Outliers can significantly impact coefficient estimates\n",
    "- **Limited Flexibility**: May underfit complex relationships in the data\n",
    "- **Collinearity Issues**: Performance degrades with highly correlated features\n",
    "\n",
    "## Choosing the Right Base Learner\n",
    "\n",
    "The optimal base learner depends on your specific use case:\n",
    "\n",
    "- **RandomForestRegressor**: Good default choice that balances flexibility and robustness\n",
    "- **LGBMRegressor/XGBRegressor**: Best for large datasets where prediction accuracy is paramount\n",
    "- **LinearRegression**: Ideal when interpretability is critical or when the treatment effect is known to be linear\n",
    "\n",
    "For complex, heterogeneous treatment effects, tree-based methods (Random Forest, LightGBM, XGBoost) typically outperform linear models. However, linear models offer better interpretability and explicit modeling of treatment interactions.\n",
    "</div>\n"
   ],
   "id": "792220a69865f63a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T04:23:33.217298Z",
     "start_time": "2025-06-19T04:23:32.916941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize T-Learner with different base models\n",
    "t_learner_rf = TLearner(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "t_learner_linear = TLearner(LinearRegression())\n",
    "\n",
    "# Create LightGBM model if available\n",
    "if 'LIGHTGBM_AVAILABLE' in globals() and LIGHTGBM_AVAILABLE:\n",
    "    t_learner_lgbm = TLearner(LGBMRegressor(random_state=42))\n",
    "    # Uncomment to use LightGBM model\n",
    "    # tl = t_learner_lgbm\n",
    "\n",
    "# Create XGBoost model if available\n",
    "if 'XGBOOST_AVAILABLE' in globals() and XGBOOST_AVAILABLE:\n",
    "    t_learner_xgb = TLearner(XGBRegressor(random_state=42))\n",
    "    # Uncomment to use XGBoost model\n",
    "    # tl = t_learner_xgb\n",
    "\n",
    "# Select Random Forest as our primary model\n",
    "tl = t_learner_rf\n",
    "\n",
    "# Generate synthetic data with heterogeneous treatment effects if not already defined\n",
    "if 'features' not in globals():\n",
    "    # Create an instance of synthetic_data_for_cate with model2\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "    # Generate synthetic data\n",
    "    features, treatment_vector, outcomes = data_generator.get_synthetic_data()\n",
    "    print(f\"Generated data with {features.shape[0]} samples and {features.shape[1]} features\")\n",
    "    print(f\"Treatment assignment rate: {treatment_vector.mean():.2f}\")\n",
    "    print(f\"Outcome mean: {outcomes.mean():.2f}\")\n",
    "else:\n",
    "    # If features already exist, create a data generator to calculate true CATE\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting T-Learner with Random Forest...\")\n",
    "tl.fit(\n",
    "    X=features,             # Feature matrix\n",
    "    t=treatment_vector,     # Treatment assignments (0/1)\n",
    "    y=outcomes              # Observed outcomes\n",
    ")\n",
    "print(\"Model fitting complete.\")\n"
   ],
   "id": "36322d4a19888638",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Estimation\n",
    "\n",
    "After fitting the model, we can estimate the Conditional Average Treatment Effect (CATE) for each individual in our dataset. The CATE represents how much the treatment is expected to affect the outcome for an individual with specific characteristics.\n",
    "\n",
    "The CATE estimation process involves:\n",
    "1. Creating two copies of each individual's features, one with treatment=1 and one with treatment=0\n",
    "2. Predicting outcomes for both scenarios\n",
    "3. Computing the difference between the predicted outcomes (treatment - control)\n",
    "\n",
    "This gives us an estimate of the treatment effect for each individual, conditional on their covariates.\n",
    "</div>\n"
   ],
   "id": "2905e8c000580ab1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T04:23:33.258957Z",
     "start_time": "2025-06-19T04:23:33.231398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate synthetic data with heterogeneous treatment effects if not already defined\n",
    "if 'features' not in globals():\n",
    "    # Create an instance of synthetic_data_for_cate with model2\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "    # Generate synthetic data\n",
    "    features, treatment_vector, outcomes = data_generator.get_synthetic_data()\n",
    "    print(f\"Generated data with {features.shape[0]} samples and {features.shape[1]} features\")\n",
    "    print(f\"Treatment assignment rate: {treatment_vector.mean():.2f}\")\n",
    "    print(f\"Outcome mean: {outcomes.mean():.2f}\")\n",
    "else:\n",
    "    # If features already exist, create a data generator to calculate true CATE\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "# Estimate treatment effects (CATE)\n",
    "cate = tl.effect(X=features)\n",
    "\n",
    "# Print basic statistics about the estimated CATE\n",
    "print(f\"CATE Statistics:\")\n",
    "print(f\"  Mean: {cate.mean():.4f}\")\n",
    "print(f\"  Std Dev: {cate.std():.4f}\")\n",
    "print(f\"  Min: {cate.min():.4f}\")\n",
    "print(f\"  Max: {cate.max():.4f}\")\n"
   ],
   "id": "fcfb2cd053fc027c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Distribution Visualization\n",
    "\n",
    "Visualizing the distribution of estimated treatment effects helps us understand the heterogeneity in treatment effects across the population. This can reveal:\n",
    "\n",
    "1. **Average Effect**: The center of the distribution shows the average treatment effect\n",
    "2. **Effect Heterogeneity**: The spread of the distribution shows how much treatment effects vary\n",
    "3. **Subgroups**: Multiple peaks might indicate distinct subgroups with different responses\n",
    "4. **Negative/Positive Effects**: The proportion of individuals with negative vs. positive effects\n",
    "\n",
    "A narrow distribution suggests homogeneous treatment effects, while a wide distribution suggests high heterogeneity. Skewness in the distribution might indicate that certain types of individuals benefit more or less from the treatment.\n",
    "</div>\n"
   ],
   "id": "2e7882e3d25b4e96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T04:23:34.540842Z",
     "start_time": "2025-06-19T04:23:33.273656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate true treatment effects using the data generator\n",
    "# Generate synthetic data with heterogeneous treatment effects if not already defined\n",
    "if 'features' not in globals():\n",
    "    # Create an instance of synthetic_data_for_cate with model2\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "    # Generate synthetic data\n",
    "    features, treatment_vector, outcomes = data_generator.get_synthetic_data()\n",
    "    print(f\"Generated data with {features.shape[0]} samples and {features.shape[1]} features\")\n",
    "    print(f\"Treatment assignment rate: {treatment_vector.mean():.2f}\")\n",
    "    print(f\"Outcome mean: {outcomes.mean():.2f}\")\n",
    "else:\n",
    "    # If features already exist, create a data generator to calculate true CATE\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "# Calculate true treatment effects using the data generator\n",
    "true_cate = data_generator.get_true_cate(features)\n",
    "\n",
    "# Separate treated and control groups\n",
    "treated_indices = treatment_vector == 1\n",
    "control_indices = treatment_vector == 0\n",
    "\n",
    "# Create DataFrames for each group\n",
    "import pandas as pd\n",
    "\n",
    "treated_df = pd.DataFrame({\n",
    "    'Estimated CATE': cate[treated_indices],\n",
    "    'True CATE': true_cate[treated_indices]\n",
    "})\n",
    "\n",
    "control_df = pd.DataFrame({\n",
    "    'Estimated CATE': cate[control_indices],\n",
    "    'True CATE': true_cate[control_indices]\n",
    "})\n",
    "\n",
    "# Create images directory if it doesn't exist\n",
    "import os\n",
    "\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "# Plot for Treatment Group\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot both distributions with KDE\n",
    "sns.histplot(data=treated_df['Estimated CATE'], kde=True, alpha=0.6, bins=30)\n",
    "\n",
    "# Add vertical lines for mean values\n",
    "plt.axvline(x=treated_df['Estimated CATE'].mean(), color='blue', linestyle='--', linewidth=2,\n",
    "            label=f'Mean Estimated CATE: {treated_df[\"Estimated CATE\"].mean():.4f}')\n",
    "plt.axvline(x=treated_df['True CATE'].mean(), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f'Mean True CATE: {treated_df[\"True CATE\"].mean():.4f}')\n",
    "\n",
    "# Add vertical line at zero (no effect)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1,\n",
    "            label='No Effect')\n",
    "\n",
    "# Add annotations\n",
    "plt.title('Distribution of True vs Estimated CATE - Treatment Group', fontsize=14)\n",
    "plt.xlabel('Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show percentage of positive and negative effects for estimated CATE\n",
    "est_pos_pct = (treated_df['Estimated CATE'] > 0).mean() * 100\n",
    "est_neg_pct = (treated_df['Estimated CATE'] < 0).mean() * 100\n",
    "plt.annotate(f'Estimated Positive Effects: {est_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.90), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'Estimated Negative Effects: {est_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.85), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "# Show percentage of positive and negative effects for true CATE\n",
    "true_pos_pct = (treated_df['True CATE'] > 0).mean() * 100\n",
    "true_neg_pct = (treated_df['True CATE'] < 0).mean() * 100\n",
    "plt.annotate(f'True Positive Effects: {true_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.80), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'True Negative Effects: {true_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.75), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display plot\n",
    "plt.savefig('images/t_learner_cate_distribution_treatment.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Treatment group CATE distribution plot saved to images/t_learner_cate_distribution_treatment.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot for Control Group\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot both distributions with KDE\n",
    "sns.histplot(data=control_df['Estimated CATE'], kde=True, alpha=0.6, bins=30)\n",
    "\n",
    "# Add vertical lines for mean values\n",
    "plt.axvline(x=control_df['Estimated CATE'].mean(), color='blue', linestyle='--', linewidth=2,\n",
    "            label=f'Mean Estimated CATE: {control_df[\"Estimated CATE\"].mean():.4f}')\n",
    "plt.axvline(x=control_df['True CATE'].mean(), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f'Mean True CATE: {control_df[\"True CATE\"].mean():.4f}')\n",
    "\n",
    "# Add vertical line at zero (no effect)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1,\n",
    "            label='No Effect')\n",
    "\n",
    "# Add annotations\n",
    "plt.title('Distribution of True vs Estimated CATE - Control Group', fontsize=14)\n",
    "plt.xlabel('Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show percentage of positive and negative effects for estimated CATE\n",
    "est_pos_pct = (control_df['Estimated CATE'] > 0).mean() * 100\n",
    "est_neg_pct = (control_df['Estimated CATE'] < 0).mean() * 100\n",
    "plt.annotate(f'Estimated Positive Effects: {est_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.90), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'Estimated Negative Effects: {est_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.85), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "# Show percentage of positive and negative effects for true CATE\n",
    "true_pos_pct = (control_df['True CATE'] > 0).mean() * 100\n",
    "true_neg_pct = (control_df['True CATE'] < 0).mean() * 100\n",
    "plt.annotate(f'True Positive Effects: {true_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.80), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'True Negative Effects: {true_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.75), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display plot\n",
    "plt.savefig('images/t_learner_cate_distribution_control.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Control group CATE distribution plot saved to images/t_learner_cate_distribution_control.png\")\n",
    "plt.show()\n",
    "\n",
    "# Combined plot (original)\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Create a DataFrame for easier plotting with seaborn\n",
    "cate_df = pd.DataFrame({\n",
    "    'Estimated CATE': cate,\n",
    "    'True CATE': true_cate\n",
    "})\n",
    "\n",
    "# Plot both distributions with KDE\n",
    "sns.histplot(data=cate_df['Estimated CATE'], kde=True, alpha=0.6, bins=30)\n",
    "\n",
    "# Add vertical lines for mean values\n",
    "plt.axvline(x=cate.mean(), color='blue', linestyle='--', linewidth=2,\n",
    "            label=f'Mean Estimated CATE: {cate.mean():.4f}')\n",
    "plt.axvline(x=true_cate.mean(), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f'Mean True CATE: {true_cate.mean():.4f}')\n",
    "\n",
    "# Add vertical line at zero (no effect)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1,\n",
    "            label='No Effect')\n",
    "\n",
    "# Add annotations\n",
    "plt.title('Distribution of True vs Estimated CATE - All Groups', fontsize=14)\n",
    "plt.xlabel('Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show percentage of positive and negative effects for estimated CATE\n",
    "est_pos_pct = (cate > 0).mean() * 100\n",
    "est_neg_pct = (cate < 0).mean() * 100\n",
    "plt.annotate(f'Estimated Positive Effects: {est_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.90), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'Estimated Negative Effects: {est_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.85), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "# Show percentage of positive and negative effects for true CATE\n",
    "true_pos_pct = (true_cate > 0).mean() * 100\n",
    "true_neg_pct = (true_cate < 0).mean() * 100\n",
    "plt.annotate(f'True Positive Effects: {true_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.80), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'True Negative Effects: {true_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.75), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display plot\n",
    "plt.savefig('images/t_learner_cate_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Combined CATE distribution plot saved to images/t_learner_cate_distribution.png\")\n",
    "plt.show()\n"
   ],
   "id": "c57878b07df43a89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Overlaid CATE Distributions: Estimated vs. True\n",
    "\n",
    "To better compare the estimated and true CATE distributions, we can overlay them on the same plot using different colors. This visualization allows us to:\n",
    "\n",
    "1. **Directly Compare Shapes**: See how closely the estimated distribution matches the true distribution\n",
    "2. **Identify Discrepancies**: Spot areas where the model over- or under-estimates treatment effects\n",
    "3. **Assess Heterogeneity Capture**: Determine if the model captures the true heterogeneity in treatment effects\n",
    "4. **Evaluate Peaks and Modes**: Compare the peaks and modes of both distributions\n",
    "\n",
    "The plot below shows both distributions with different colors, allowing for a direct visual comparison of their shapes, centers, and spreads.\n",
    "</div>\n"
   ],
   "id": "f530e61bc2146e95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T04:23:34.930546Z",
     "start_time": "2025-06-19T04:23:34.584418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a plot that overlays both the estimated and true CATE distributions\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot both distributions with KDE using different colors and line styles\n",
    "sns.kdeplot(data=cate_df, x='Estimated CATE', color='blue', fill=False, linewidth=2.5, linestyle='-',\n",
    "            label=f'Estimated CATE (Mean: {cate.mean():.4f})')\n",
    "sns.kdeplot(data=cate_df, x='True CATE', color='red', fill=False, linewidth=2.5, linestyle='--',\n",
    "            label=f'True CATE (Mean: {true_cate.mean():.4f})')\n",
    "\n",
    "# Add vertical lines for mean values\n",
    "plt.axvline(x=cate.mean(), color='blue', linestyle='-', linewidth=2)\n",
    "plt.axvline(x=true_cate.mean(), color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add vertical line at zero (no effect)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1, label='No Effect')\n",
    "\n",
    "# Add annotations\n",
    "plt.title('Comparison of Estimated vs. True CATE Distributions', fontsize=14)\n",
    "plt.xlabel('Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate and display statistics\n",
    "est_std = cate.std()\n",
    "true_std = true_cate.std()\n",
    "\n",
    "# Calculate correlation and mean absolute error\n",
    "correlation = np.corrcoef(true_cate, cate)[0, 1]\n",
    "mae = np.mean(np.abs(true_cate - cate))\n",
    "\n",
    "# Safely calculate KL divergence approximation to avoid division by zero and log of negative values\n",
    "# Only consider positive values and avoid division by zero\n",
    "valid_indices = (cate > 0) & (true_cate > 0)\n",
    "if np.any(valid_indices):\n",
    "    kl_div = np.sum(cate[valid_indices] * np.log(cate[valid_indices] / true_cate[valid_indices]))\n",
    "else:\n",
    "    kl_div = np.nan\n",
    "\n",
    "# Add statistics annotations\n",
    "plt.annotate(f'Estimated CATE Std: {est_std:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'True CATE Std: {true_std:.4f}', xy=(0.05, 0.90), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'Mean Absolute Error: {mae:.4f}', xy=(0.05, 0.85), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'Correlation: {correlation:.4f}', xy=(0.05, 0.80), xycoords='axes fraction', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display plot\n",
    "plt.savefig('images/t_learner_cate_distributions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"CATE distributions comparison plot saved to images/t_learner_cate_distributions_comparison.png\")\n",
    "plt.show()\n"
   ],
   "id": "4df9344059bfb5bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Accuracy Evaluation: Predicted vs. Actual Treatment Effects\n",
    "\n",
    "To evaluate the accuracy of our T-Learner model, we can compare the predicted CATE values with the actual treatment effects. This comparison helps us:\n",
    "\n",
    "1. **Assess Model Accuracy**: How well does the model recover the true treatment effects?\n",
    "2. **Identify Patterns in Errors**: Are there systematic biases in the predictions?\n",
    "3. **Compare Treatment Groups**: Do predictions differ in accuracy between treated and control groups?\n",
    "4. **Detect Outliers**: Are there individuals for whom the model predictions are particularly inaccurate?\n",
    "\n",
    "The scatter plot below shows:\n",
    "- Predicted CATE values on the y-axis\n",
    "- True treatment effects on the x-axis\n",
    "- Different symbols for treated and control groups\n",
    "- A diagonal line representing perfect prediction (y=x)\n",
    "\n",
    "Points close to the diagonal line indicate accurate predictions, while deviations suggest estimation errors.\n",
    "</div>\n"
   ],
   "id": "1c72a8322b80d89a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T04:23:35.271342Z",
     "start_time": "2025-06-19T04:23:34.947507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We already calculated the true CATE earlier, so we'll use that\n",
    "\n",
    "# Create a scatter plot of predicted vs. true treatment effects\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Separate treated and control groups for different markers\n",
    "treated_indices = treatment_vector == 1\n",
    "control_indices = treatment_vector == 0\n",
    "\n",
    "# Plot treated group with one marker\n",
    "plt.scatter(true_cate[treated_indices], cate[treated_indices], \n",
    "            marker='^', color='red', alpha=0.6, label='Treated Group')\n",
    "\n",
    "# Plot control group with another marker\n",
    "plt.scatter(true_cate[control_indices], cate[control_indices], \n",
    "            marker='o', color='blue', alpha=0.6, label='Control Group')\n",
    "\n",
    "# Add diagonal line (perfect prediction)\n",
    "min_val = min(true_cate.min(), cate.min())\n",
    "max_val = max(true_cate.max(), cate.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Perfect Prediction')\n",
    "\n",
    "# Calculate and display correlation coefficient\n",
    "# Note: correlation is already calculated in the previous cell\n",
    "plt.annotate(f'Correlation: {correlation:.4f}', \n",
    "             xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12)\n",
    "\n",
    "# Calculate and display mean absolute error\n",
    "# Note: mae is already calculated in the previous cell\n",
    "plt.annotate(f'Mean Absolute Error: {mae:.4f}', \n",
    "             xy=(0.05, 0.90), xycoords='axes fraction', fontsize=12)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Predicted vs. Actual Treatment Effects', fontsize=14)\n",
    "plt.xlabel('Actual Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Predicted Treatment Effect (CATE)', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('images/t_learner_cate_accuracy_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"CATE accuracy evaluation plot saved to images/t_learner_cate_accuracy_evaluation.png\")\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "3d655ac0144345d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Comparison of T-Learner and S-Learner Performance\n",
    "\n",
    "Visual inspection of the scatter plots and overlaid distributions in both the T-Learner and S-Learner notebooks reveals a striking similarity in their performance on this synthetic data for 'model2'. Despite their different theoretical approaches, both models achieve comparable results in terms of:\n",
    "\n",
    "1. **Correlation with True CATE**: Both models show a strong positive correlation with the true CATE values\n",
    "2. **Mean Absolute Error**: The average magnitude of errors is similar between the two approaches\n",
    "3. **Distribution Shape**: The estimated CATE distributions have similar shapes and spread\n",
    "4. **Prediction Patterns**: Both models show similar patterns in their scatter plots, with comparable clustering around the diagonal line\n",
    "\n",
    "## Why T-Learner Isn't Superior Despite Its Theoretical Advantages\n",
    "\n",
    "In theory, the T-Learner should have advantages over the S-Learner in certain scenarios:\n",
    "\n",
    "1. **Separate Models**: By training separate models for treatment and control groups, T-Learner can capture different functional forms for each group\n",
    "2. **No Treatment Variable Importance Issues**: Unlike S-Learner, T-Learner doesn't rely on the treatment indicator being given sufficient importance\n",
    "3. **Flexibility**: T-Learner can adapt to different response surfaces in the treatment and control groups\n",
    "\n",
    "However, for this specific synthetic data ('model2'), these advantages don't translate to superior performance because:\n",
    "\n",
    "1. **Sufficient Data in Both Groups**: The synthetic data has enough samples in both treatment and control groups, allowing the S-Learner to learn the treatment effect properly\n",
    "2. **Random Forest Base Learner**: The RandomForestRegressor used in both approaches is flexible enough to capture the complex relationships, even when the treatment is just another feature\n",
    "3. **Threshold-Based Effects**: The true CATE in 'model2' is based on threshold functions that create distinct subgroups with different effects, which both models can capture equally well\n",
    "4. **Similar Functional Forms**: The underlying functional forms for treatment and control outcomes may not be different enough to benefit from T-Learner's separate modeling approach\n",
    "\n",
    "## Implications\n",
    "\n",
    "This similarity in performance suggests that:\n",
    "\n",
    "1. **Model Choice Flexibility**: For this type of data, either approach could be used with similar results\n",
    "2. **Computational Efficiency**: S-Learner might be preferred for its simplicity and computational efficiency (training one model instead of two)\n",
    "3. **Data-Dependent Performance**: The relative performance of these meta-learners is highly dependent on the specific data characteristics\n",
    "4. **Need for Model Comparison**: It's valuable to compare multiple approaches rather than relying on theoretical advantages alone\n",
    "\n",
    "This observation aligns with findings in causal inference literature that no single meta-learner consistently outperforms others across all datasets and scenarios.\n",
    "</div>\n"
   ],
   "id": "6c65cf8efc562bba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}