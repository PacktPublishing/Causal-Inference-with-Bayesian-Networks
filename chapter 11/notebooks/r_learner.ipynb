{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# R-Learner for Conditional Average Treatment Effect (CATE) Estimation\n",
    "\n",
    "This notebook demonstrates the use of R-Learner for estimating heterogeneous treatment effects. \n",
    "\n",
    "## What is R-Learner?\n",
    "\n",
    "R-Learner (Residual-Learner) is a meta-learner approach for estimating Conditional Average Treatment Effects (CATE). The key characteristics of R-Learner are:\n",
    "\n",
    "1. **Residualization Approach**: Uses residuals from outcome and treatment models\n",
    "2. **Orthogonalization**: Separates treatment effect estimation from confounding\n",
    "3. **Doubly Robust**: Provides consistent estimates if either the outcome model or propensity model is correct\n",
    "4. **Flexible Base Learners**: Can use any regression model that follows scikit-learn's API\n",
    "5. **Efficient Estimation**: Often achieves lower variance than other meta-learners\n",
    "\n",
    "## How R-Learner Works\n",
    "\n",
    "The R-Learner approach follows these steps:\n",
    "1. Fit models to predict outcomes and treatment assignment\n",
    "2. Compute residuals from both models\n",
    "3. Regress outcome residuals on treatment residuals to estimate treatment effects\n",
    "\n",
    "The main advantage of R-Learner is its efficiency in estimating treatment effects, especially when either the outcome model or propensity model is correctly specified. It often achieves lower variance than other meta-learners and can handle continuous treatments naturally.\n",
    "</div>\n"
   ],
   "id": "97be6c6dc64d089b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Library Imports\n",
    "\n",
    "We import the necessary libraries for this demonstration:\n",
    "\n",
    "- **synthetic_data_for_cate2**: Custom module for generating synthetic data with enhanced heterogeneity for treatment effects\n",
    "- **RLearner**: Our implementation of the R-Learner meta-learner\n",
    "- **sklearn models**: Various regression models to use as base learners\n",
    "- **matplotlib/seaborn**: For visualization\n",
    "- **numpy**: For numerical operations\n",
    "\n",
    "We also attempt to import optional dependencies (LightGBM and XGBoost) which provide additional base learners.\n",
    "</div>\n"
   ],
   "id": "dfcb66631716c240"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T20:58:13.940209Z",
     "start_time": "2025-06-30T20:58:13.920796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "from synthetic_data.synthetic_data_for_cate_class import synthetic_data_for_cate\n",
    "# Force reload of the r_learner_improved module to ensure we're using the latest version\n",
    "# This is an improved implementation that properly handles sparse matrices and has better performance\n",
    "import metalearners.r_learner_improved\n",
    "importlib.reload(metalearners.r_learner_improved)\n",
    "from metalearners.r_learner_improved import RLearnerImproved as RLearner\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set plot style\n",
    "try:\n",
    "    # For newer matplotlib versions (>=3.6)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except ValueError:\n",
    "    # For older matplotlib versions\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Try to import optional dependencies\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not available. Some examples will be skipped.\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Some examples will be skipped.\")\n"
   ],
   "id": "7522d5e575206fb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Note on Dependencies\n",
    "\n",
    "This notebook requires the following packages:\n",
    "- numpy, matplotlib, seaborn: For data manipulation and visualization\n",
    "- scikit-learn: For machine learning models\n",
    "- lightgbm, xgboost (optional): For gradient boosting models\n",
    "\n",
    "The notebook also uses local packages:\n",
    "- synthetic_data.synthetic_data_for_cate_class: For generating synthetic data with enhanced heterogeneity\n",
    "- metalearners.r_learner_improved: For the improved RLearner implementation\n",
    "\n",
    "Make sure you have installed all required dependencies before running this notebook.\n",
    "</div>\n"
   ],
   "id": "8dd6f2183794ea21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Synthetic Data Generation\n",
    "\n",
    "We generate synthetic data with known heterogeneous treatment effects using the `generate_synthetic_data_for_cate2()` function. This function creates:\n",
    "\n",
    "- A feature matrix with 5 covariates (by default)\n",
    "- A binary treatment indicator (1=treated, 0=control)\n",
    "- An outcome variable that depends on both covariates and treatment\n",
    "\n",
    "The data generation process includes:\n",
    "- Non-linear confounding (treatment assignment depends on X1 and X2)\n",
    "- Heterogeneous treatment effects (effects vary based on all covariates)\n",
    "- Non-linear baseline effects (outcome depends non-linearly on covariates)\n",
    "- Heteroskedastic noise (noise level varies with X1)\n",
    "\n",
    "This synthetic data allows us to evaluate the performance of different CATE estimation methods, as we know the true treatment effects.\n",
    "</div>\n"
   ],
   "id": "1a4e47b40927554f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T20:58:13.960755Z",
     "start_time": "2025-06-30T20:58:13.950066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an instance of synthetic_data_for_cate with model2\n",
    "data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "# Generate synthetic data with heterogeneous treatment effects\n",
    "features, treatment_vector, outcomes = data_generator.get_synthetic_data()\n",
    "\n",
    "# Print basic information about the generated data\n",
    "print(f\"Generated data with {features.shape[0]} samples and {features.shape[1]} features\")\n",
    "print(f\"Treatment assignment rate: {treatment_vector.mean():.2f}\")\n",
    "print(f\"Outcome mean: {outcomes.mean():.2f}\")\n"
   ],
   "id": "c5e603d9d95f8bff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Model Initialization and Fitting\n",
    "\n",
    "We initialize several R-Learner models with different base learners:\n",
    "\n",
    "1. **Random Forest**: A non-parametric model that can capture complex non-linear relationships\n",
    "2. **Linear Regression**: A simple parametric model that assumes linear relationships\n",
    "3. **Gradient Boosting**: An ensemble method that builds trees sequentially to correct errors\n",
    "4. **LightGBM** (if available): A gradient boosting framework that uses tree-based learning\n",
    "5. **XGBoost** (if available): Another gradient boosting framework with different implementation\n",
    "\n",
    "Each model has different strengths and weaknesses for CATE estimation. For this demonstration, we'll use the Random Forest-based R-Learner as our primary model.\n",
    "\n",
    "The fitting process involves:\n",
    "1. Splitting the data into training and validation sets\n",
    "2. Fitting outcome and treatment models on the training set\n",
    "3. Computing residuals for both outcomes and treatments on the validation set\n",
    "4. Fitting the effect model on the ratio of residuals\n",
    "</div>\n"
   ],
   "id": "54893785b064b16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Base Learners for R-Learner: Principles, Pros, and Cons\n",
    "\n",
    "The choice of base learner significantly impacts the performance of R-Learner for CATE estimation. Here we describe the basic ideas, advantages, and limitations of each base learner.\n",
    "\n",
    "## RandomForestRegressor\n",
    "\n",
    "### Basic Principles\n",
    "- **Ensemble Method**: Combines multiple decision trees to improve prediction accuracy and control overfitting\n",
    "- **Bootstrap Aggregating (Bagging)**: Each tree is trained on a random subset of the data with replacement\n",
    "- **Feature Randomization**: At each split, only a random subset of features is considered\n",
    "- **Averaging**: Final prediction is the average of predictions from all trees\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **Captures Non-linear Relationships**: Can model complex, non-linear treatment effects without explicit specification\n",
    "- **Handles Interactions**: Automatically captures interactions between treatment and covariates\n",
    "- **Robust to Outliers**: Less sensitive to extreme values in the data\n",
    "- **No Distributional Assumptions**: Does not assume normality or homoscedasticity\n",
    "- **Feature Importance**: Provides insights into which covariates drive treatment effect heterogeneity\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Black Box**: Less interpretable than linear models\n",
    "- **Treatment Variable Importance**: May not give sufficient importance to the treatment indicator\n",
    "- **Computational Cost**: Training many trees can be computationally intensive\n",
    "- **Hyperparameter Sensitivity**: Performance depends on proper tuning of hyperparameters\n",
    "- **Extrapolation Limitations**: May not extrapolate well to regions with sparse data\n",
    "\n",
    "## GradientBoostingRegressor\n",
    "\n",
    "### Basic Principles\n",
    "- **Sequential Ensemble**: Builds trees sequentially, with each tree correcting errors of previous trees\n",
    "- **Gradient Descent**: Uses gradient descent to minimize a loss function\n",
    "- **Weak Learners**: Each tree is a simple model (often shallow trees)\n",
    "- **Additive Model**: Final prediction is the sum of predictions from all trees\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **High Accuracy**: Often achieves excellent prediction performance\n",
    "- **Handles Non-linearity**: Can capture complex, non-linear treatment effects\n",
    "- **Feature Importance**: Provides insights into which covariates drive treatment effect heterogeneity\n",
    "- **Regularization Options**: Various parameters to control overfitting\n",
    "- **Robust to Outliers**: Less sensitive to extreme values than linear models\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Overfitting Risk**: Can easily overfit without proper regularization\n",
    "- **Computational Cost**: Sequential nature makes it slower than random forests\n",
    "- **Hyperparameter Sensitivity**: Performance depends heavily on proper tuning\n",
    "- **Black Box**: Less interpretable than linear models\n",
    "- **Memory Usage**: Can be memory-intensive for large datasets\n",
    "\n",
    "## LGBMRegressor (LightGBM)\n",
    "\n",
    "### Basic Principles\n",
    "- **Gradient Boosting Framework**: Builds trees sequentially, with each tree correcting errors of previous trees\n",
    "- **Leaf-wise Growth**: Grows trees by leaf-wise (best-first) rather than level-wise growth\n",
    "- **Histogram-based Learning**: Buckets continuous features into discrete bins to speed up training\n",
    "- **Gradient-based One-Side Sampling (GOSS)**: Focuses on instances with larger gradients during training\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **High Performance**: Often achieves state-of-the-art prediction accuracy\n",
    "- **Efficiency**: Faster training speed and lower memory usage than traditional gradient boosting\n",
    "- **Handles Large Datasets**: Scales well to datasets with many observations\n",
    "- **Regularization Options**: Built-in L1/L2 regularization to prevent overfitting\n",
    "- **Categorical Feature Support**: Native handling of categorical variables\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Complex Tuning**: Requires careful hyperparameter tuning for optimal performance\n",
    "- **Overfitting Risk**: Can overfit on small datasets without proper regularization\n",
    "- **Less Robust to Noisy Data**: May be more sensitive to noise than Random Forests\n",
    "- **Treatment Importance**: Like Random Forests, may not give sufficient importance to treatment indicator\n",
    "- **Installation Challenges**: Requires additional system dependencies\n",
    "\n",
    "## XGBRegressor (XGBoost)\n",
    "\n",
    "### Basic Principles\n",
    "- **Extreme Gradient Boosting**: Enhanced implementation of gradient boosting\n",
    "- **Regularized Learning**: Includes L1 and L2 regularization terms in the objective function\n",
    "- **Approximate Greedy Algorithm**: Uses a quantile sketch algorithm to find approximate best splits\n",
    "- **Sparsity-Aware Split Finding**: Efficiently handles missing values and sparse data\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **Prediction Accuracy**: Consistently strong performance across many prediction tasks\n",
    "- **Regularization**: Built-in mechanisms to prevent overfitting\n",
    "- **Handles Missing Values**: Native support for missing data\n",
    "- **Parallel Processing**: Efficient computation using multi-threading\n",
    "- **Cross-validation**: Built-in cross-validation capabilities\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Complexity**: Many hyperparameters to tune\n",
    "- **Black Box Nature**: Limited interpretability of the model\n",
    "- **Memory Usage**: Can be memory-intensive for large datasets\n",
    "- **Treatment Variable Importance**: May not prioritize treatment indicator appropriately\n",
    "- **Installation Requirements**: Depends on additional libraries\n",
    "\n",
    "## LinearRegression\n",
    "\n",
    "### Basic Principles\n",
    "- **Linear Function**: Models outcome as a weighted sum of input features\n",
    "- **Ordinary Least Squares (OLS)**: Minimizes the sum of squared differences between observed and predicted values\n",
    "- **Closed-form Solution**: Parameters can be calculated directly without iterative optimization\n",
    "- **Additive Effects**: Assumes features contribute independently to the outcome\n",
    "\n",
    "### Pros for CATE Estimation\n",
    "- **Interpretability**: Clear interpretation of coefficients, including treatment effect\n",
    "- **Treatment Interaction Modeling**: Can explicitly model interactions between treatment and covariates\n",
    "- **Computational Efficiency**: Fast to train and make predictions\n",
    "- **Stability**: Results are stable and reproducible\n",
    "- **Statistical Inference**: Provides p-values and confidence intervals for treatment effects\n",
    "\n",
    "### Cons for CATE Estimation\n",
    "- **Linearity Assumption**: Cannot capture non-linear treatment effects without manual feature engineering\n",
    "- **Homogeneity Assumption**: Assumes constant error variance across all observations\n",
    "- **Sensitivity to Outliers**: Outliers can significantly impact coefficient estimates\n",
    "- **Limited Flexibility**: May underfit complex relationships in the data\n",
    "- **Collinearity Issues**: Performance degrades with highly correlated features\n",
    "\n",
    "## Choosing the Right Base Learner\n",
    "\n",
    "The optimal base learner depends on your specific use case:\n",
    "\n",
    "- **RandomForestRegressor**: Good default choice that balances flexibility and robustness\n",
    "- **GradientBoostingRegressor**: Good for capturing complex treatment effects with potentially better accuracy\n",
    "- **LGBMRegressor/XGBRegressor**: Best for large datasets where prediction accuracy is paramount\n",
    "- **LinearRegression**: Ideal when interpretability is critical or when the treatment effect is known to be linear\n",
    "\n",
    "For complex, heterogeneous treatment effects, tree-based methods (Random Forest, Gradient Boosting, LightGBM, XGBoost) typically outperform linear models. However, linear models offer better interpretability and explicit modeling of treatment interactions.\n",
    "</div>\n"
   ],
   "id": "3341d9318275820c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T20:58:14.758734Z",
     "start_time": "2025-06-30T20:58:14.098418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize R-Learner with different base models and optimal settings\n",
    "# Note: RLearnerImproved uses more complex models by default:\n",
    "# - RandomForestRegressor with n_estimators=200, max_depth=15, min_samples_leaf=5\n",
    "# - use_propensity_for_prediction=False (performs better in tests)\n",
    "\n",
    "# Create R-Learner instances with different model combinations\n",
    "r_learner_rf = RLearner(\n",
    "    # Using default RandomForestRegressor with enhanced parameters\n",
    "    use_propensity_for_prediction=False  # This setting performed best in tests\n",
    ")\n",
    "\n",
    "# Custom linear model\n",
    "linear_model = LinearRegression()\n",
    "r_learner_linear = RLearner(\n",
    "    outcome_model=linear_model,\n",
    "    treatment_model=linear_model,\n",
    "    effect_model=linear_model,\n",
    "    use_propensity_for_prediction=False\n",
    ")\n",
    "\n",
    "# Custom gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "r_learner_gb = RLearner(\n",
    "    outcome_model=gb_model,\n",
    "    treatment_model=gb_model,\n",
    "    effect_model=gb_model,\n",
    "    use_propensity_for_prediction=False\n",
    ")\n",
    "\n",
    "# Create LightGBM model if available\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    try:\n",
    "        lgbm_model = LGBMRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "        r_learner_lgbm = RLearner(\n",
    "            outcome_model=lgbm_model,\n",
    "            treatment_model=lgbm_model,\n",
    "            effect_model=lgbm_model,\n",
    "            use_propensity_for_prediction=False  # This setting performed best in tests\n",
    "        )\n",
    "    except AttributeError:\n",
    "        print(\"LightGBM model creation failed\")\n",
    "        LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "# Create XGBoost model if available \n",
    "if XGBOOST_AVAILABLE:\n",
    "    try:\n",
    "        xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "        r_learner_xgb = RLearner(\n",
    "            outcome_model=xgb_model,\n",
    "            treatment_model=xgb_model,\n",
    "            effect_model=xgb_model,\n",
    "            use_propensity_for_prediction=False  # This setting performed best in tests\n",
    "        )\n",
    "    except AttributeError:\n",
    "        print(\"XGBoost model creation failed\")\n",
    "        XGBOOST_AVAILABLE = False\n",
    "\n",
    "# Select Random Forest as our primary model\n",
    "rl = r_learner_rf\n",
    "\n",
    "# Convert features to dense array if it's sparse\n",
    "try:\n",
    "    print(\"Fitting R-Learner with Random Forest...\")\n",
    "    # Check if features is a sparse matrix (has toarray method)\n",
    "    if hasattr(features, 'toarray'):\n",
    "        features_dense = features.toarray()\n",
    "    else:\n",
    "        features_dense = features\n",
    "\n",
    "    rl.fit(\n",
    "        X=features_dense,  # Dense feature matrix\n",
    "        y=outcomes,  # Observed outcomes \n",
    "        treatment=treatment_vector  # Treatment assignments (0/1)\n",
    "    )\n",
    "    print(\"Model fitting complete.\")\n",
    "except AttributeError as e:\n",
    "    print(f\"Error fitting model: {str(e)}\")\n"
   ],
   "id": "19bb4456a5862126",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Estimation\n",
    "\n",
    "After fitting the model, we can estimate the Conditional Average Treatment Effect (CATE) for each individual in our dataset. The CATE represents how much the treatment is expected to affect the outcome for an individual with specific characteristics.\n",
    "\n",
    "The CATE estimation process for R-Learner involves:\n",
    "1. Using the effect model trained on residuals to directly predict treatment effects\n",
    "2. The effect model has learned to estimate treatment effects from the relationship between outcome residuals and treatment residuals\n",
    "\n",
    "This gives us an estimate of the treatment effect for each individual, conditional on their covariates.\n",
    "</div>\n"
   ],
   "id": "82b3959e577ba27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T20:58:14.787986Z",
     "start_time": "2025-06-30T20:58:14.771896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate synthetic data with heterogeneous treatment effects if not already defined\n",
    "if 'features' not in globals():\n",
    "    # Create an instance of synthetic_data_for_cate with model2\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "    # Generate synthetic data\n",
    "    features, treatment_vector, outcomes = data_generator.get_synthetic_data()\n",
    "    print(f\"Generated data with {features.shape[0]} samples and {features.shape[1]} features\")\n",
    "    print(f\"Treatment assignment rate: {treatment_vector.mean():.2f}\")\n",
    "    print(f\"Outcome mean: {outcomes.mean():.2f}\")\n",
    "else:\n",
    "    # If features already exist, create a data generator to calculate true CATE\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "# Estimate treatment effects (CATE)\n",
    "# Convert features to dense array if it's sparse\n",
    "if hasattr(features, 'toarray'):\n",
    "    features_dense = features.toarray()\n",
    "else:\n",
    "    features_dense = features\n",
    "\n",
    "cate = rl.predict(X=features_dense)\n",
    "\n",
    "# Print basic statistics about the estimated CATE\n",
    "print(f\"CATE Statistics:\")\n",
    "print(f\"  Mean: {cate.mean():.4f}\")\n",
    "print(f\"  Std Dev: {cate.std():.4f}\")\n",
    "print(f\"  Min: {cate.min():.4f}\")\n",
    "print(f\"  Max: {cate.max():.4f}\")\n"
   ],
   "id": "25c6183181f6dae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Distribution Visualization\n",
    "\n",
    "Visualizing the distribution of estimated treatment effects helps us understand the heterogeneity in treatment effects across the population. This can reveal:\n",
    "\n",
    "1. **Average Effect**: The center of the distribution shows the average treatment effect\n",
    "2. **Effect Heterogeneity**: The spread of the distribution shows how much treatment effects vary\n",
    "3. **Subgroups**: Multiple peaks might indicate distinct subgroups with different responses\n",
    "4. **Negative/Positive Effects**: The proportion of individuals with negative vs. positive effects\n",
    "\n",
    "A narrow distribution suggests homogeneous treatment effects, while a wide distribution suggests high heterogeneity. Skewness in the distribution might indicate that certain types of individuals benefit more or less from the treatment.\n",
    "</div>\n"
   ],
   "id": "cba978ba264ca36a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T20:58:15.239651Z",
     "start_time": "2025-06-30T20:58:14.806229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate true treatment effects using the data generator\n",
    "# Generate synthetic data with heterogeneous treatment effects if not already defined\n",
    "if 'features' not in globals():\n",
    "    # Create an instance of synthetic_data_for_cate with model2\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "    # Generate synthetic data\n",
    "    features, treatment_vector, outcomes = data_generator.get_synthetic_data()\n",
    "    print(f\"Generated data with {features.shape[0]} samples and {features.shape[1]} features\")\n",
    "    print(f\"Treatment assignment rate: {treatment_vector.mean():.2f}\")\n",
    "    print(f\"Outcome mean: {outcomes.mean():.2f}\")\n",
    "else:\n",
    "    # If features already exist, create a data generator to calculate true CATE\n",
    "    data_generator = synthetic_data_for_cate(model_type='model2')\n",
    "\n",
    "# Calculate true treatment effects using the data generator\n",
    "# Ensure features is a dense array\n",
    "if hasattr(features, 'toarray'):\n",
    "    features_dense = features.toarray()\n",
    "else:\n",
    "    features_dense = features\n",
    "\n",
    "true_cate = data_generator.get_true_cate(features_dense)\n",
    "\n",
    "# Create a DataFrame for easier plotting with seaborn\n",
    "import pandas as pd\n",
    "cate_df = pd.DataFrame({\n",
    "    'Estimated CATE': cate,\n",
    "    'True CATE': true_cate\n",
    "})\n",
    "\n",
    "# Create images directory if it doesn't exist\n",
    "import os\n",
    "# Create images directory in the project root if it doesn't exist\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))  # Go up one level from notebooks directory\n",
    "images_dir = os.path.join(project_root, 'images')\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# Plot the distribution of estimated CATE\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot both distributions with KDE\n",
    "sns.histplot(data=cate_df['Estimated CATE'], kde=True, alpha=0.6, bins=30)\n",
    "\n",
    "# Add vertical lines for mean values\n",
    "plt.axvline(x=cate.mean(), color='blue', linestyle='--', linewidth=2,\n",
    "            label=f'Mean Estimated CATE: {cate.mean():.4f}')\n",
    "plt.axvline(x=true_cate.mean(), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f'Mean True CATE: {true_cate.mean():.4f}')\n",
    "\n",
    "# Add vertical line at zero (no effect)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1,\n",
    "            label='No Effect')\n",
    "\n",
    "# Add annotations\n",
    "plt.title('Distribution of Estimated CATE - R-Learner', fontsize=14)\n",
    "plt.xlabel('Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show percentage of positive and negative effects for estimated CATE\n",
    "est_pos_pct = (cate > 0).mean() * 100\n",
    "est_neg_pct = (cate < 0).mean() * 100\n",
    "plt.annotate(f'Estimated Positive Effects: {est_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.90), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'Estimated Negative Effects: {est_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.85), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "# Show percentage of positive and negative effects for true CATE\n",
    "true_pos_pct = (true_cate > 0).mean() * 100\n",
    "true_neg_pct = (true_cate < 0).mean() * 100\n",
    "plt.annotate(f'True Positive Effects: {true_pos_pct:.1f}%',\n",
    "             xy=(0.68, 0.80), xycoords='axes fraction', fontsize=11)\n",
    "plt.annotate(f'True Negative Effects: {true_neg_pct:.1f}%',\n",
    "             xy=(0.68, 0.75), xycoords='axes fraction', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display plot\n",
    "plt.savefig(os.path.join(images_dir, 'r_learner_cate_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"CATE distribution plot saved to {os.path.join(images_dir, 'r_learner_cate_distribution.png')}\")\n",
    "plt.show()\n"
   ],
   "id": "73e8212848c40e46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# Overlaid CATE Distributions: Estimated vs. True\n",
    "\n",
    "To better compare the estimated and true CATE distributions, we can overlay them on the same plot using different colors. This visualization allows us to:\n",
    "\n",
    "1. **Directly Compare Shapes**: See how closely the estimated distribution matches the true distribution\n",
    "2. **Identify Discrepancies**: Spot areas where the model over- or under-estimates treatment effects\n",
    "3. **Assess Heterogeneity Capture**: Determine if the model captures the true heterogeneity in treatment effects\n",
    "4. **Evaluate Peaks and Modes**: Compare the peaks and modes of both distributions\n",
    "\n",
    "The plot below shows both distributions with different colors, allowing for a direct visual comparison of their shapes, centers, and spreads.\n",
    "</div>\n"
   ],
   "id": "76c159377f46e4b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T20:58:15.595444Z",
     "start_time": "2025-06-30T20:58:15.263889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a plot that overlays both the estimated and true CATE distributions\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot both distributions with KDE using different colors and line styles\n",
    "sns.kdeplot(data=cate_df, x='Estimated CATE', color='blue', fill=False, linewidth=2.5, linestyle='-',\n",
    "            label=f'Estimated CATE (Mean: {cate.mean():.4f})')\n",
    "sns.kdeplot(data=cate_df, x='True CATE', color='red', fill=False, linewidth=2.5, linestyle='--',\n",
    "            label=f'True CATE (Mean: {true_cate.mean():.4f})')\n",
    "\n",
    "# Add vertical lines for mean values\n",
    "plt.axvline(x=cate.mean(), color='blue', linestyle='-', linewidth=2)\n",
    "plt.axvline(x=true_cate.mean(), color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add vertical line at zero (no effect)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1, label='No Effect')\n",
    "\n",
    "# Add annotations\n",
    "plt.title('Comparison of Estimated vs. True CATE Distributions - R-Learner', fontsize=14)\n",
    "plt.xlabel('Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate and display statistics\n",
    "est_std = cate.std()\n",
    "true_std = true_cate.std()\n",
    "\n",
    "# Calculate correlation and mean absolute error\n",
    "correlation = np.corrcoef(true_cate, cate)[0, 1]\n",
    "mae = np.mean(np.abs(true_cate - cate))\n",
    "\n",
    "# Add statistics annotations\n",
    "plt.annotate(f'Estimated CATE Std: {est_std:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'True CATE Std: {true_std:.4f}', xy=(0.05, 0.90), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'Mean Absolute Error: {mae:.4f}', xy=(0.05, 0.85), xycoords='axes fraction', fontsize=10)\n",
    "plt.annotate(f'Correlation: {correlation:.4f}', xy=(0.05, 0.80), xycoords='axes fraction', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display plot\n",
    "plt.savefig(os.path.join(images_dir, 'r_learner_cate_distributions_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"CATE distributions comparison plot saved to {os.path.join(images_dir, 'r_learner_cate_distributions_comparison.png')}\")\n",
    "plt.show()\n"
   ],
   "id": "5ad3b98618261fd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"font-size: 0.85em;\">\n",
    "\n",
    "# CATE Accuracy Evaluation: Predicted vs. Actual Treatment Effects\n",
    "\n",
    "To evaluate the accuracy of our R-Learner model, we can compare the predicted CATE values with the actual treatment effects. This comparison helps us:\n",
    "\n",
    "1. **Assess Model Accuracy**: How well does the model recover the true treatment effects?\n",
    "2. **Identify Patterns in Errors**: Are there systematic biases in the predictions?\n",
    "3. **Compare Treatment Groups**: Do predictions differ in accuracy between treated and control groups?\n",
    "4. **Detect Outliers**: Are there individuals for whom the model predictions are particularly inaccurate?\n",
    "\n",
    "The scatter plot below shows:\n",
    "- Predicted CATE values on the y-axis\n",
    "- True treatment effects on the x-axis\n",
    "- Different symbols for treated and control groups\n",
    "- A diagonal line representing perfect prediction (y=x)\n",
    "\n",
    "Points close to the diagonal line indicate accurate predictions, while deviations suggest estimation errors.\n",
    "</div>\n"
   ],
   "id": "50599d8276003d70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T20:58:15.916231Z",
     "start_time": "2025-06-30T20:58:15.611145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a scatter plot of predicted vs. true treatment effects\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Separate treated and control groups for different markers\n",
    "treated_indices = treatment_vector == 1\n",
    "control_indices = treatment_vector == 0\n",
    "\n",
    "# Plot treated group with one marker\n",
    "plt.scatter(true_cate[treated_indices], cate[treated_indices], \n",
    "            marker='^', color='red', alpha=0.6, label='Treated Group')\n",
    "\n",
    "# Plot control group with another marker\n",
    "plt.scatter(true_cate[control_indices], cate[control_indices], \n",
    "            marker='o', color='blue', alpha=0.6, label='Control Group')\n",
    "\n",
    "# Add diagonal line (perfect prediction)\n",
    "min_val = min(true_cate.min(), cate.min())\n",
    "max_val = max(true_cate.max(), cate.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Perfect Prediction')\n",
    "\n",
    "# Calculate and display correlation coefficient\n",
    "plt.annotate(f'Correlation: {correlation:.4f}', \n",
    "             xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12)\n",
    "\n",
    "# Calculate and display mean absolute error\n",
    "plt.annotate(f'Mean Absolute Error: {mae:.4f}', \n",
    "             xy=(0.05, 0.90), xycoords='axes fraction', fontsize=12)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Predicted vs. Actual Treatment Effects - R-Learner', fontsize=14)\n",
    "plt.xlabel('Actual Treatment Effect', fontsize=12)\n",
    "plt.ylabel('Predicted Treatment Effect (CATE)', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(os.path.join(images_dir, 'r_learner_cate_accuracy_evaluation.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"CATE accuracy evaluation plot saved to {os.path.join(images_dir, 'r_learner_cate_accuracy_evaluation.png')}\")\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "b8091a2b2e8401d1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}